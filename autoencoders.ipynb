{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yanch\\.conda\\envs\\cov-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with N = 100\n",
      "  Seed 1/1\n",
      "Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yanch\\.conda\\envs\\cov-env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64, 4096])) that is different to the input size (torch.Size([64, 1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (4096) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 287\u001b[0m\n\u001b[0;32m    284\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# set to 0 for uncorrelated gaussian data, 0.4 for correlated gaussian data\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Run the experiment for different N values\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_seeds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 220\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(N_values, num_epochs, seeds)\u001b[0m\n\u001b[0;32m    217\u001b[0m vae_model \u001b[38;5;241m=\u001b[39m VariationalAutoencoder(image_size\u001b[38;5;241m=\u001b[39mimage_size, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mae_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m train_vae(vae_model, train_loader, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Get reconstruction errors\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 114\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[1;34m(model, dataloader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[0;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    113\u001b[0m output \u001b[38;5;241m=\u001b[39m model(img)\n\u001b[1;32m--> 114\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    116\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\yanch\\.conda\\envs\\cov-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yanch\\.conda\\envs\\cov-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yanch\\.conda\\envs\\cov-env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yanch\\.conda\\envs\\cov-env\\lib\\site-packages\\torch\\nn\\functional.py:3338\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\yanch\\.conda\\envs\\cov-env\\lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (4096) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from src.get_data import *\n",
    "from src.models import *\n",
    "from src.utils import *\n",
    "from src.nd import *\n",
    "from datetime import datetime\n",
    "\n",
    "save_dir = os.path.join('results', 'autoencoders')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Define Autoencoder and Variational Autoencoder with one hidden layer\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, image_size, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, image_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        x_recon = x_recon.view(x.size(0), 1, int(np.sqrt(image_size)), int(np.sqrt(image_size)))\n",
    "        return x_recon\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, image_size, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(image_size, latent_dim)\n",
    "        self.fc_mu = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(latent_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, image_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc_mu(h1), self.fc_logvar(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.sigmoid(self.fc3(z))\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        x_recon = x_recon.view(x.size(0), 1, int(np.sqrt(image_size)), int(np.sqrt(image_size)))\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Generate a training set of N images\n",
    "def get_datasets(N, seed, dataset='gaussian', b=0, batch_size=64, device='cpu'):\n",
    "    dimension = int(np.sqrt(image_size))\n",
    "    if dataset == \"gaussian\":\n",
    "            cov = np.ones((dimension, dimension)) * b\n",
    "            np.fill_diagonal(cov, 1)\n",
    "            L = np.linalg.cholesky(cov)\n",
    "            fam = np.random.randn(N, dimension) @ L\n",
    "            nov = np.random.randn(N, dimension) @ L\n",
    "            X = torch.from_numpy(fam).float()\n",
    "            X_test = torch.from_numpy(nov).float()\n",
    "    elif dataset == \"tinyimagenet\":\n",
    "        (X, _), (X_test, _) = get_tiny_imagenet(\n",
    "            \"./data\",\n",
    "            sample_size=N,\n",
    "            sample_size_test=N,\n",
    "            batch_size=batch_size,\n",
    "            seed=seed,\n",
    "            device=device,\n",
    "        )\n",
    "        X = X.reshape((X.shape[0], -1)).float()\n",
    "        X_test = X_test.reshape((X_test.shape[0], -1)).float()\n",
    "    return X, X_test\n",
    "    \n",
    "\n",
    "# Step 3: Train both models on the training set\n",
    "def train_autoencoder(model, dataloader, num_epochs=100, learning_rate=1e-3, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # for img, _ in dataloader:\n",
    "        for img in dataloader:\n",
    "            img = img.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img)\n",
    "            loss = loss_fn(output, img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"[AE] Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), losses, label='Autoencoder Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Autoencoder Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def train_vae(model, dataloader, num_epochs=5, learning_rate=1e-3, device='cpu'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for img, _ in dataloader:\n",
    "            img = img.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(img)\n",
    "            loss = vae_loss_function(recon_batch, img, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"[VAE] Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), losses, label='VAE Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Variational Autoencoder Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    recon_x = recon_x.view(x.size(0), -1)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "# Run a forward pass on both sets of images, collect errors\n",
    "def get_reconstruction_errors(model, dataloader, device='cpu', model_type='ae'):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for img, _ in dataloader:\n",
    "            img = img.to(device)\n",
    "            if model_type == 'ae':\n",
    "                output = model(img)\n",
    "            elif model_type == 'vae':\n",
    "                output, _, _ = model(img)\n",
    "            else:\n",
    "                raise ValueError(\"model_type must be 'ae' or 'vae'\")\n",
    "            # Compute reconstruction error per sample\n",
    "            recon_error = F.mse_loss(output, img, reduction='none')\n",
    "            recon_error = recon_error.view(recon_error.size(0), -1).mean(dim=1)\n",
    "            errors.extend(recon_error.cpu().numpy())\n",
    "    return np.array(errors)\n",
    "\n",
    "# Step 6: Compare the two arrays using the provided function\n",
    "def compare_novelty(energy_nov, energy_fam):\n",
    "    # Compare the judgments for each of the two\n",
    "    energy_comparison = energy_nov - energy_fam\n",
    "    # If an entry is positive, then the subject has correctly identified the novel picture from that pair\n",
    "    return np.where(energy_comparison >= 0, 1, 0)\n",
    "\n",
    "# Step 7: Repeat the experiment for different N and plot the accuracy for each\n",
    "def run_experiment(N_values, num_epochs=100, seeds=5):\n",
    "    accuracies_ae = []\n",
    "    accuracies_vae = []\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    for N in N_values:\n",
    "        print(f\"\\nRunning experiment with N = {N}\")\n",
    "        acc_ae_seeds = []\n",
    "        acc_vae_seeds = []\n",
    "        for seed in range(seeds):\n",
    "            print(f\"  Seed {seed+1}/{seeds}\")\n",
    "            # Prepare datasets\n",
    "            train_dataset, novel_dataset = get_datasets(N, seed, dataset, b, batch_size=64, device=device)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "            novel_loader = DataLoader(novel_dataset, batch_size=64, shuffle=False)\n",
    "            fam_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "            \n",
    "            # Initialize models\n",
    "            latent_dim = d_latent\n",
    "            \n",
    "            print(f\"Device: {device}\")\n",
    "            ae_model = Autoencoder(image_size=image_size, latent_dim=latent_dim)\n",
    "            vae_model = VariationalAutoencoder(image_size=image_size, latent_dim=latent_dim)\n",
    "            \n",
    "            # Train models\n",
    "            train_autoencoder(ae_model, train_loader, num_epochs=num_epochs, device=device)\n",
    "            train_vae(vae_model, train_loader, num_epochs=num_epochs, device=device)\n",
    "            \n",
    "            # Get reconstruction errors\n",
    "            errors_fam_ae = get_reconstruction_errors(ae_model, fam_loader, device=device, model_type='ae')\n",
    "            errors_nov_ae = get_reconstruction_errors(ae_model, novel_loader, device=device, model_type='ae')\n",
    "            errors_fam_vae = get_reconstruction_errors(vae_model, fam_loader, device=device, model_type='vae')\n",
    "            errors_nov_vae = get_reconstruction_errors(vae_model, novel_loader, device=device, model_type='vae')\n",
    "            \n",
    "            # Compare errors and compute accuracy\n",
    "            comparisons_ae = compare_novelty(errors_nov_ae, errors_fam_ae)\n",
    "            accuracy_ae = comparisons_ae.mean()\n",
    "            acc_ae_seeds.append(accuracy_ae)\n",
    "            print(f\"  [AE] Accuracy: {accuracy_ae:.4f}\")\n",
    "            \n",
    "            comparisons_vae = compare_novelty(errors_nov_vae, errors_fam_vae)\n",
    "            accuracy_vae = comparisons_vae.mean()\n",
    "            acc_vae_seeds.append(accuracy_vae)\n",
    "            print(f\"  [VAE] Accuracy: {accuracy_vae:.4f}\")\n",
    "        \n",
    "        accuracies_ae.append((np.mean(acc_ae_seeds), np.std(acc_ae_seeds)))\n",
    "        accuracies_vae.append((np.mean(acc_vae_seeds), np.std(acc_vae_seeds)))\n",
    "    \n",
    "    # Plot the accuracies with error bars\n",
    "    ae_means, ae_stds = zip(*accuracies_ae)\n",
    "    vae_means, vae_stds = zip(*accuracies_vae)\n",
    "    print(f\"Autoencoder: {ae_means}, {ae_stds}\")\n",
    "    print(f\"Variational Autoencoder: {vae_means}, {vae_stds}\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.errorbar(N_values, ae_means, yerr=ae_stds, label='Autoencoder', fmt='-o')\n",
    "    plt.errorbar(N_values, vae_means, yerr=vae_stds, label='Variational Autoencoder', fmt='-o')\n",
    "    plt.xlabel('Number of Samples (N)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Novelty Detection Accuracy vs Number of Samples')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # plt.savefig(os.path.join(save_dir, 'autoencoders_accuracy_vs_N.pdf'))\n",
    "\n",
    "    # Save the results\n",
    "    np.savez(\n",
    "        os.path.join(save_dir, f'means.npz'), \n",
    "        ae_means=ae_means,\n",
    "        vae_means=vae_means,\n",
    "    )\n",
    "    np.savez(\n",
    "        os.path.join(save_dir, f'stds.npz'),\n",
    "        ae_stds=ae_stds,\n",
    "        vae_stds=vae_stds, \n",
    "    )\n",
    "\n",
    "\n",
    "# N_values = [20, 40, 100, 200, 400, 1000, 4000, 10000]\n",
    "N_values = [100, 1000]\n",
    "num_seeds = 1\n",
    "\n",
    "# dataset = 'gaussian' # 'gaussian' or 'tinyimagenet'\n",
    "dataset = 'tinyimagenet'\n",
    "size_dict = {\"gaussian\": 500, \"mnist\": 784, \"tinyimagenet\": 4096}\n",
    "image_size = size_dict[dataset]\n",
    "d_latent = 200  # latent dimension\n",
    "b = 0 # set to 0 for uncorrelated gaussian data, 0.4 for correlated gaussian data\n",
    "\n",
    "# Run the experiment for different N values\n",
    "run_experiment(N_values=N_values, num_epochs=200, seeds=num_seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Define the models\n",
    "image_size = 28 * 28  # MNIST images\n",
    "latent_dim = 400\n",
    "\n",
    "ae_model = Autoencoder(image_size=image_size, latent_dim=latent_dim)\n",
    "vae_model = VariationalAutoencoder(image_size=image_size, latent_dim=latent_dim)\n",
    "\n",
    "# Count parameters\n",
    "ae_params = count_parameters(ae_model)\n",
    "vae_params = count_parameters(vae_model)\n",
    "\n",
    "print(f\"Number of trainable parameters in Autoencoder: {ae_params}\")\n",
    "print(f\"Number of trainable parameters in Variational Autoencoder: {vae_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cov-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
